{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Similar Word Prediction\n",
        "Word2Vec model development using HuggingFace dataset for predicting similar words, enhancing text analysis and NLP applications with accurate word embeddings."
      ],
      "metadata": {
        "id": "oYhV1YeBSCUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Libraries"
      ],
      "metadata": {
        "id": "4oedM-1CZY1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "BIO7l0BZ4ONV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "mM2LLvmFeDVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datasets import load_dataset\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "yVhSW1v_EGwO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(['punkt', 'stopwords', 'wordnet'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A21ru6UNHCk9",
        "outputId": "25031069-62c5-42ef-e48b-9233f0c37c18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Dataset"
      ],
      "metadata": {
        "id": "bmJsA49ieLl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = 'sentence-transformers/all-nli'\n",
        "config = 'pair-score'\n",
        "\n",
        "dataset = load_dataset(dataset_id, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuAhQr8F4swm",
        "outputId": "6c1e6097-e522-43a7-b73c-bc0418f3b324"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Dataset"
      ],
      "metadata": {
        "id": "dxzN8naIeQ8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFcntWXi9qPi",
        "outputId": "da7431c8-bf19-4b0b-8b5a-eee51209e189"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (942069, 3), 'dev': (19657, 3), 'test': (19656, 3)}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentence1 = dataset['train']['sentence1']\n",
        "train_sentence2 = dataset['train']['sentence2']\n",
        "test_sentence1 = dataset['test']['sentence1']\n",
        "test_sentence2 = dataset['test']['sentence2']\n",
        "dev_sentence1 = dataset['dev']['sentence1']\n",
        "dev_sentence2 = dataset['dev']['sentence2']"
      ],
      "metadata": {
        "id": "LnLNzVTDCFuD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sentences = train_sentence1 + train_sentence2 + test_sentence1 + test_sentence2 + dev_sentence1 + dev_sentence2\n",
        "all_sentences = list(set(all_sentences))"
      ],
      "metadata": {
        "id": "LqOs4oFVCFq5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8828lf2cCFoc",
        "outputId": "5431adc1-4fc7-46bc-db59-15c55ee4b5b6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1196336"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "id": "vjvxuZxKed5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create linguistic_preprocessing function\n",
        "def linguistic_preprocessing(text):\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove stop words\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "    # Lemmatize the tokens\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "id": "RoYKmkMtD7iO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = []\n",
        "for sent in tqdm(all_sentences):\n",
        "  sentences.append(linguistic_preprocessing(sent))\n",
        "\n",
        "sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OptS_kWnELsQ",
        "outputId": "e5da3c4c-b79b-4882-b016-a3d897f616fc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1196336/1196336 [28:35<00:00, 697.26it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['dad', 'cherishes', 'new', 'baby', '.'],\n",
              " ['group',\n",
              "  'people',\n",
              "  'gather',\n",
              "  'round',\n",
              "  'coffe',\n",
              "  'table',\n",
              "  'costume',\n",
              "  'party',\n",
              "  '.'],\n",
              " ['man', 'walking', 'street', 'past', 'colorful', 'house', '.'],\n",
              " ['hud',\n",
              "  'decided',\n",
              "  'take',\n",
              "  'step',\n",
              "  'towards',\n",
              "  'correcting',\n",
              "  'final',\n",
              "  'rule',\n",
              "  '.'],\n",
              " ['everybody', 'knew', 'hijacking', '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Word2Vec Model"
      ],
      "metadata": {
        "id": "m3KsKSj8ei66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Word2Vec model with hyperparameters\n",
        "model = Word2Vec(sentences, vector_size=300, window=5, min_count=10, sg=0, epochs=5)"
      ],
      "metadata": {
        "id": "-kCY1igzO0dQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting Similar Words"
      ],
      "metadata": {
        "id": "D7tCYxj-enjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_word(word):\n",
        "  try:\n",
        "    word = word.lower()\n",
        "    similar_words = model.wv.most_similar(word, topn=5)\n",
        "    for word, score in similar_words:\n",
        "      print(f\"{word.capitalize()}: {score:.2f}\")\n",
        "  except:\n",
        "    print('No similar word found in vocabulary!')"
      ],
      "metadata": {
        "id": "xj8DZDgFcZJp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_similar_word('Cricket')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUTiulXZes2G",
        "outputId": "90c90d40-6cdb-4500-c5e7-e2367497f344"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rugby: 0.84\n",
            "Badminton: 0.84\n",
            "Lacrosse: 0.84\n",
            "Ping-pong: 0.80\n",
            "Dice: 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_similar_word('Book')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtkhOWfId0Yh",
        "outputId": "e7804925-8e31-46a5-96c0-0617ae2e97eb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Novel: 0.65\n",
            "Textbook: 0.63\n",
            "Magazine: 0.61\n",
            "Newspaper: 0.60\n",
            "Bible: 0.58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_similar_word('Dog')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFe-RlypdGzt",
        "outputId": "b88da784-f2ab-418a-c0ef-aee4a58677ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Puppy: 0.83\n",
            "Poodle: 0.81\n",
            "Cat: 0.75\n",
            "Chihuahua: 0.65\n",
            "Canine: 0.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RXH_2yYOcOqb"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}